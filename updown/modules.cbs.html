
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8" />

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-120523111-2"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-120523111-2');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Inconsolata&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Ubuntu+Mono&display=swap" rel="stylesheet">


    <title>updown.modules.cbs &#8212; updown 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="updown.modules.updown_cell" href="modules.updown_cell.html" />
    <link rel="prev" title="updown.modules.attention" href="modules.attention.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="module-updown.modules.cbs">
<span id="updown-modules-cbs"></span><h1>updown.modules.cbs<a class="headerlink" href="#module-updown.modules.cbs" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="updown.modules.cbs.ConstrainedBeamSearch">
<em class="property">class </em><code class="sig-prename descclassname">updown.modules.cbs.</code><code class="sig-name descname">ConstrainedBeamSearch</code><span class="sig-paren">(</span><em class="sig-param">end_index: int</em>, <em class="sig-param">max_steps: int = 20</em>, <em class="sig-param">beam_size: int = 5</em>, <em class="sig-param">per_node_beam_size: Optional[int] = None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/nocaps-org/updown-baseline/blob/master/updown/modules/cbs.py#L20-L277"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#updown.modules.cbs.ConstrainedBeamSearch" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Implements Constrained Beam Search for decoding the most likely sequences conditioned on a
Finite State Machine with specified state transitions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We keep the method signatures as close to <code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSearch</span></code>
as possible. Most of the docstring is adapted from AllenNLP, so thanks to them!</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>end_index</strong><span class="classifier">int</span></dt><dd><p>The index of the <code class="docutils literal notranslate"><span class="pre">&#64;&#64;BOUNDARY&#64;&#64;</span></code> token in the target vocabulary.</p>
</dd>
<dt><strong>max_steps</strong><span class="classifier">int, optional (default = 20)</span></dt><dd><p>The maximum number of decoding steps to take, i.e. the maximum length of the predicted
sequences.</p>
</dd>
<dt><strong>beam_size</strong><span class="classifier">int, optional (default = 10)</span></dt><dd><p>The width of the beam used for each “main state” in the Finite State Machine.</p>
</dd>
<dt><strong>per_node_beam_size</strong><span class="classifier">int, optional (default = beam_size)</span></dt><dd><p>The maximum number of candidates to consider per node, at each step in the search.
If not given, this just defaults to <code class="docutils literal notranslate"><span class="pre">beam_size</span></code>. Setting this parameter
to a number smaller than <code class="docutils literal notranslate"><span class="pre">beam_size</span></code> may give better results, as it can introduce
more diversity into the search. See <a class="reference external" href="https://arxiv.org/abs/1702.01806">Beam Search Strategies for Neural Machine Translation.
Freitag and Al-Onaizan, 2017</a>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="updown.modules.cbs.ConstrainedBeamSearch.search">
<code class="sig-name descname">search</code><span class="sig-paren">(</span><em class="sig-param">self, start_predictions:torch.Tensor, start_state:Dict[str, torch.Tensor], step:Callable[[torch.Tensor, Dict[str, torch.Tensor]], Tuple[torch.Tensor, Dict[str, torch.Tensor]]], fsm:torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference external" href="https://github.com/nocaps-org/updown-baseline/blob/master/updown/modules/cbs.py#L59-L277"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#updown.modules.cbs.ConstrainedBeamSearch.search" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a starting state, a step function, and an FSM adjacency matrix, apply Constrained
Beam Search to find most likely target sequences satisfying specified constraints in FSM.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If your step function returns <code class="docutils literal notranslate"><span class="pre">-inf</span></code> for some log probabilities
(like if you’re using a masked log-softmax) then some of the “best”
sequences returned may also have <code class="docutils literal notranslate"><span class="pre">-inf</span></code> log probability. Specifically
this happens when the beam size is smaller than the number of actions
with finite log probability (non-zero probability) returned by the step function.
Therefore if you’re using a mask you may want to check the results from <code class="docutils literal notranslate"><span class="pre">search</span></code>
and potentially discard sequences with non-finite log probability.</p>
</div>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>start_predictions</strong><span class="classifier">torch.Tensor</span></dt><dd><p>A tensor containing the initial predictions with shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">)</span></code>. These are
usually just <code class="docutils literal notranslate"><span class="pre">&#64;&#64;BOUNDARY&#64;&#64;</span></code> token indices.</p>
</dd>
<dt><strong>start_state</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code></span></dt><dd><p>The initial state passed to the <code class="docutils literal notranslate"><span class="pre">step</span></code> function. Each value of the state dict
should be a tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">*)</span></code>, where <code class="docutils literal notranslate"><span class="pre">*</span></code> means any other
number of dimensions.</p>
</dd>
<dt><strong>step</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">StepFunctionType</span></code></span></dt><dd><p>A function that is responsible for computing the next most likely tokens, given the
current state and the predictions from the last time step. The function should accept
two arguments. The first being a tensor of shape <code class="docutils literal notranslate"><span class="pre">(group_size,)</span></code>, representing the
index of the predicted tokens from the last time step, and the second being the
current state. The <code class="docutils literal notranslate"><span class="pre">group_size</span></code> will be <code class="docutils literal notranslate"><span class="pre">batch_size</span> <span class="pre">*</span> <span class="pre">beam_size</span> <span class="pre">*</span> <span class="pre">num_fsm_states</span></code>
except in the initial step, for which it will just be <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>. The function is
expected to return a tuple, where the first element is a tensor of shape
<code class="docutils literal notranslate"><span class="pre">(group_size,</span> <span class="pre">vocab_size)</span></code> containing the log probabilities of the tokens for the
next step, and the second element is the updated state. The tensor in the state should
have shape <code class="docutils literal notranslate"><span class="pre">(group_size,</span> <span class="pre">*)</span></code>, where <code class="docutils literal notranslate"><span class="pre">*</span></code> means any other number of dimensions.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>Tuple[torch.Tensor, torch.Tensor]</dt><dd><p>Tuple of <code class="docutils literal notranslate"><span class="pre">(predictions,</span> <span class="pre">log_probabilities)</span></code>, where <code class="docutils literal notranslate"><span class="pre">predictions</span></code>
has shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_fsm_states,</span> <span class="pre">beam_size,</span> <span class="pre">max_steps)</span></code>
and <code class="docutils literal notranslate"><span class="pre">log_probabilities</span></code> has shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_fsm_states,</span> <span class="pre">beam_size)</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">updown</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../setup_dependencies.html">How to setup this codebase?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">How to train your captioner?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../evaluation_inference.html">How to evaluate or do inference?</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="config.html">updown.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">updown.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">updown.models</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">updown.modules</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="modules.attention.html">updown.modules.attention</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">updown.modules.cbs</a></li>
<li class="toctree-l2"><a class="reference internal" href="modules.updown_cell.html">updown.modules.updown_cell</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">updown.utils</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="modules.html">updown.modules</a><ul>
      <li>Previous: <a href="modules.attention.html" title="previous chapter">updown.modules.attention</a></li>
      <li>Next: <a href="modules.updown_cell.html" title="next chapter">updown.modules.updown_cell</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, nocaps team.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/updown/modules.cbs.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>